{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e8d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f833u/fySegger/lib/python3.11/site-packages/anndata/_core/anndata.py:1776: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "WARNING:root:Number of missing genes: 127\n"
     ]
    }
   ],
   "source": [
    "from segger.data.parquet.sample import STSampleParquet\n",
    "from path import Path\n",
    "from segger.data.utils import calculate_gene_celltype_abundance_embedding\n",
    "import scanpy as sc\n",
    "from segger.data.parquet._utils import get_polygons_from_xy\n",
    "from segger.data.parquet.sample import STSampleParquet\n",
    "from segger.training.segger_data_module import SeggerDataModule\n",
    "from segger.training.train import LitSegger\n",
    "#from segger.prediction.predict_parquet import segment, load_model\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch import Trainer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from segger.models.segger_model import Segger\n",
    "from torch_geometric.nn import to_hetero\n",
    "\n",
    "\n",
    "SCRNASEQ_FILE='/dkfz/cluster/gpu/data/OE0606/fengyun/xenium_data/xenium_colon/scRNAseq.h5ad'\n",
    "XENIUM_DATA_DIR='/dkfz/cluster/gpu/data/OE0606/fengyun/xenium_data/xenium_colon'\n",
    "CELLTYPE_COLUMN = \"Level1\"\n",
    "\n",
    "# Base directory to store Pytorch Lightning models\n",
    "xenium_data_dir = '/dkfz/cluster/gpu/data/OE0606/fengyun/xenium_data/xenium_colon'\n",
    "segger_data_dir = '/dkfz/cluster/gpu/data/OE0606/fengyun/xenium_data/segger_colon'\n",
    "\n",
    "scrnaseq = sc.read(SCRNASEQ_FILE)\n",
    "sc.pp.subsample(scrnaseq, 0.1)\n",
    "scrnaseq.var_names_make_unique()\n",
    "# Calculate gene-celltype embeddings from reference data\n",
    "gene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(\n",
    "    scrnaseq,\n",
    "    CELLTYPE_COLUMN\n",
    ") \n",
    "\n",
    "sample = STSampleParquet(\n",
    "    base_dir=XENIUM_DATA_DIR,\n",
    "    n_workers=4,\n",
    "    sample_type=\"xenium\",\n",
    "    # scale_factor=0.8,\n",
    "    weights=gene_celltype_abundance_embedding\n",
    ")\n",
    "\n",
    "# only run once if you want to create the dataset\n",
    "# sample.save(\n",
    "#       data_dir=segger_data_dir,\n",
    "#       k_bd=3,\n",
    "#       dist_bd=15.0,\n",
    "#       k_tx=3,\n",
    "#       dist_tx=5.0,\n",
    "#       tile_width=120,\n",
    "#       tile_height=120,\n",
    "#       neg_sampling_ratio=5.0,\n",
    "#       frac=1.0,\n",
    "#       val_prob=0.1,\n",
    "#       test_prob=0.2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f4684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "# Base directory to store Pytorch Lightning models\n",
    "models_dir = '/dkfz/cluster/gpu/checkpoints/OE0606/fengyun/segger_model/segger_colon'\n",
    "\n",
    "# Initialize the Lightning data module\n",
    "dm = SeggerDataModule(\n",
    "    data_dir=segger_data_dir,\n",
    "    batch_size=3,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "# num_tx_tokens = 500\n",
    "\n",
    "# If you use custom gene embeddings, use the following two lines instead:\n",
    "num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes\n",
    "print(f\"Number of tokens: {num_tx_tokens}\")\n",
    "\n",
    "\n",
    "model = Segger(\n",
    "    num_tx_tokens=num_tx_tokens,\n",
    "    init_emb=8,\n",
    "    hidden_channels=64,\n",
    "    out_channels=16,\n",
    "    heads=4,\n",
    "    num_mid_layers=3,\n",
    ")\n",
    "model = to_hetero(model, ([\"tx\", \"bd\"], [(\"tx\", \"belongs\", \"bd\"), (\"tx\", \"neighbors\", \"tx\")]), aggr=\"sum\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b63bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`CUDAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMisconfigurationException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m ls = LitSegger(model=model)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Initialize the Lightning trainer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#accelerator='auto', # 'gpu' or 'cpu'\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m16-mixed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# set higher number if more gpus are available\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_root_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSVLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[32m     19\u001b[39m trainer.fit(\n\u001b[32m     20\u001b[39m     model=ls,\n\u001b[32m     21\u001b[39m     datamodule=dm\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fySegger/lib/python3.11/site-packages/lightning/pytorch/utilities/argparse.py:70\u001b[39m, in \u001b[36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables.items()) + \u001b[38;5;28mlist\u001b[39m(kwargs.items()))\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fySegger/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:404\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir, model_registry)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;28mself\u001b[39m._data_connector = _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[38;5;28mself\u001b[39m._accelerator_connector = \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28mself\u001b[39m._logger_connector = _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    417\u001b[39m \u001b[38;5;28mself\u001b[39m._callback_connector = _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fySegger/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:147\u001b[39m, in \u001b[36m_AcceleratorConnector.__init__\u001b[39m\u001b[34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mself\u001b[39m._accelerator_flag = \u001b[38;5;28mself\u001b[39m._choose_gpu_accelerator_backend()\n\u001b[32m    146\u001b[39m \u001b[38;5;28mself\u001b[39m._check_device_config_and_set_final_flags(devices=devices, num_nodes=num_nodes)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_parallel_devices_and_init_accelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;28mself\u001b[39m.cluster_environment: ClusterEnvironment = \u001b[38;5;28mself\u001b[39m._choose_and_init_cluster_environment()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fySegger/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:369\u001b[39m, in \u001b[36m_AcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m accelerator_cls.is_available():\n\u001b[32m    364\u001b[39m     available_accelerator = [\n\u001b[32m    365\u001b[39m         acc_str\n\u001b[32m    366\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m acc_str \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accelerator_types\n\u001b[32m    367\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m AcceleratorRegistry[acc_str][\u001b[33m\"\u001b[39m\u001b[33maccelerator\u001b[39m\u001b[33m\"\u001b[39m].is_available()\n\u001b[32m    368\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[32m    370\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccelerator_cls.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` can not run on your system\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    372\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    373\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m `Trainer`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_accelerator\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m     )\n\u001b[32m    376\u001b[39m \u001b[38;5;28mself\u001b[39m._set_devices_flag_if_auto_passed()\n\u001b[32m    377\u001b[39m \u001b[38;5;28mself\u001b[39m._devices_flag = accelerator_cls.parse_devices(\u001b[38;5;28mself\u001b[39m._devices_flag)\n",
      "\u001b[31mMisconfigurationException\u001b[39m: `CUDAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
     ]
    }
   ],
   "source": [
    "batch = dm.train[0]\n",
    "model.forward(batch.x_dict, batch.edge_index_dict)\n",
    "# Wrap the model in LitSegger\n",
    "ls = LitSegger(model=model)\n",
    "\n",
    "# Initialize the Lightning trainer\n",
    "trainer = Trainer(\n",
    "    #accelerator='auto', # 'gpu' or 'cpu'\n",
    "    accelerator='auto',\n",
    "    strategy='auto',\n",
    "    precision='16-mixed',\n",
    "    devices=1, # set higher number if more gpus are available\n",
    "    max_epochs=100,\n",
    "    default_root_dir=models_dir,\n",
    "    logger=CSVLogger(models_dir),\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "trainer.fit(\n",
    "    model=ls,\n",
    "    datamodule=dm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d5d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segger.data.parquet.sample import STSampleParquet\n",
    "from path import Path\n",
    "from segger.data.utils import calculate_gene_celltype_abundance_embedding\n",
    "import scanpy as sc\n",
    "from segger.data.parquet._utils import get_polygons_from_xy\n",
    "from segger.data.parquet.sample import STSampleParquet\n",
    "from segger.training.segger_data_module import SeggerDataModule\n",
    "from segger.training.train import LitSegger\n",
    "#from segger.prediction.predict_parquet import segment, load_model\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch import Trainer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from segger.models.segger_model import Segger\n",
    "from torch_geometric.nn import to_hetero\n",
    "\n",
    "\n",
    "# SCRNASEQ_FILE='/dkfz/cluster/gpu/data/OE0606/fengyun/xenium_data/xenium_colon/scRNAseq.h5ad'\n",
    "XENIUM_DATA_DIR='/dkfz/cluster/gpu/data/OE0606/fengyun/xenium_data/xenium_colon'\n",
    "# CELLTYPE_COLUMN = \"Level1\"\n",
    "\n",
    "# Base directory to store Pytorch Lightning models\n",
    "xenium_data_dir = '/dkfz/cluster/gpu/data/OE0606/fengyun/xenium_data/xenium_colon'\n",
    "segger_data_dir = '/dkfz/cluster/gpu/data/OE0606/fengyun/xenium_data/segger_colon_no_seq'\n",
    "\n",
    "# scrnaseq = sc.read(SCRNASEQ_FILE)\n",
    "# sc.pp.subsample(scrnaseq, 0.1)\n",
    "# scrnaseq.var_names_make_unique()\n",
    "# # Calculate gene-celltype embeddings from reference data\n",
    "# gene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(\n",
    "#     scrnaseq,\n",
    "#     CELLTYPE_COLUMN\n",
    "# )\n",
    "\n",
    "\n",
    "sample = STSampleParquet(\n",
    "    base_dir=XENIUM_DATA_DIR,\n",
    "    n_workers=4,\n",
    "    sample_type=\"xenium\",\n",
    "    # scale_factor=0.8,\n",
    "    # weights=gene_celltype_abundance_embedding\n",
    ")\n",
    "\n",
    "# only run once if you want to create the dataset\n",
    "# sample.save(\n",
    "#       data_dir=segger_data_dir,\n",
    "#       k_bd=3,\n",
    "#       dist_bd=15.0,\n",
    "#       k_tx=3,\n",
    "#       dist_tx=5.0,\n",
    "#       tile_width=120,\n",
    "#       tile_height=120,\n",
    "#       neg_sampling_ratio=5.0,\n",
    "#       frac=1.0,\n",
    "#       val_prob=0.1,\n",
    "#       test_prob=0.2,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce2cc74",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     20\u001b[39m model = Segger(\n\u001b[32m     21\u001b[39m     num_tx_tokens=num_tx_tokens,\n\u001b[32m     22\u001b[39m     init_emb=\u001b[32m8\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     num_mid_layers=\u001b[32m3\u001b[39m,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m model = to_hetero(model, ([\u001b[33m\"\u001b[39m\u001b[33mtx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbd\u001b[39m\u001b[33m\"\u001b[39m], [(\u001b[33m\"\u001b[39m\u001b[33mtx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbelongs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbd\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mtx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mneighbors\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtx\u001b[39m\u001b[33m\"\u001b[39m)]), aggr=\u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m batch = \u001b[43mdm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     31\u001b[39m model.forward(batch.x_dict, batch.edge_index_dict)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Wrap the model in LitSegger\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fySegger/lib/python3.11/site-packages/torch_geometric/data/dataset.py:291\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03mpresent).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m \u001b[33;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np.integer))\n\u001b[32m    288\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx.dim() == \u001b[32m0\u001b[39m)\n\u001b[32m    289\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray) \u001b[38;5;129;01mand\u001b[39;00m np.isscalar(idx))):\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.get(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    292\u001b[39m     data = data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(data)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mIndexError\u001b[39m: range object index out of range"
     ]
    }
   ],
   "source": [
    "# Base directory to store Pytorch Lightning models\n",
    "models_dir = '/dkfz/cluster/gpu/checkpoints/OE0606/fengyun/segger_model/segger_colon_no_seq'\n",
    "\n",
    "# Initialize the Lightning data module\n",
    "dm = SeggerDataModule(\n",
    "    data_dir=segger_data_dir,\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "num_tx_tokens = 500\n",
    "\n",
    "# If you use custom gene embeddings, use the following two lines instead:\n",
    "# num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes\n",
    "# print(f\"Number of tokens: {num_tx_tokens}\")\n",
    "\n",
    "\n",
    "model = Segger(\n",
    "    num_tx_tokens=num_tx_tokens,\n",
    "    init_emb=8,\n",
    "    hidden_channels=64,\n",
    "    out_channels=16,\n",
    "    heads=4,\n",
    "    num_mid_layers=3,\n",
    ")\n",
    "model = to_hetero(model, ([\"tx\", \"bd\"], [(\"tx\", \"belongs\", \"bd\"), (\"tx\", \"neighbors\", \"tx\")]), aggr=\"sum\")\n",
    "\n",
    "batch = dm.train[0]\n",
    "model.forward(batch.x_dict, batch.edge_index_dict)\n",
    "# Wrap the model in LitSegger\n",
    "ls = LitSegger(model=model)\n",
    "\n",
    "# Initialize the Lightning trainer\n",
    "trainer = Trainer(\n",
    "    accelerator='auto', # 'gpu' or 'cpu'\n",
    "    #accelerator='cuda',\n",
    "    strategy='auto',\n",
    "    precision='16-mixed',\n",
    "    devices=1, # set higher number if more gpus are available\n",
    "    max_epochs=100,\n",
    "    default_root_dir=models_dir,\n",
    "    logger=CSVLogger(models_dir),\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "trainer.fit(\n",
    "    model=ls,\n",
    "    datamodule=dm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13604b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fySegger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
