{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ed4db6-5234-46b1-9f38-b5883ac88946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T22:22:55.404267Z",
     "iopub.status.busy": "2024-09-11T22:22:55.403876Z",
     "iopub.status.idle": "2024-09-11T22:22:58.089917Z",
     "shell.execute_reply": "2024-09-11T22:22:58.089303Z",
     "shell.execute_reply.started": "2024-09-11T22:22:55.404248Z"
    },
    "id": "21ed4db6-5234-46b1-9f38-b5883ac88946"
   },
   "source": [
    "# **Introduction to Segger**\n",
    "\n",
    "\n",
    "**Important note (Dec 2024):** As segger is currently undergoing constant development we highly recommend installing directly via github.\n",
    "\n",
    "\n",
    "Segger is a cutting-edge cell segmentation model specifically designed for **single-molecule resolved spatial omics** datasets. It addresses the challenge of accurately segmenting individual cells in complex imaging datasets, leveraging a unique approach based on graph neural networks (GNNs).\n",
    "\n",
    "The core idea behind Segger is to model both **nuclei** and **transcripts** as graph nodes, with edges connecting them based on their spatial proximity. This allows the model to learn from the co-occurrence of nucleic and cytoplasmic molecules, resulting in more refined and accurate cell boundaries. By using spatial information and GNNs, Segger achieves state-of-the-art performance in segmenting single cells in datasets such as 10X Xenium and MERSCOPE, outperforming traditional methods like Baysor and Cellpose.\n",
    "\n",
    "Segger's workflow consists of:\n",
    "1. **Dataset creation**: Converting raw transcriptomic data into a graph-based dataset.\n",
    "2. **Training**: Training the Segger model on the graph to learn cell boundaries.\n",
    "3. **Prediction**: Using the trained model to make predictions on new datasets.\n",
    "\n",
    "This tutorial will guide you through each step of the process, ensuring you can train and apply Segger for your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XEY6CTzK0648",
   "metadata": {
    "id": "XEY6CTzK0648"
   },
   "source": [
    "Installing segger from the GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TIQnPzfx08Zr",
   "metadata": {
    "id": "TIQnPzfx08Zr"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/EliHei2/segger_dev.git\n",
    "%cd segger_dev\n",
    "!pip install \".[rapids12]\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3SNnImS09_N",
   "metadata": {
    "id": "q3SNnImS09_N"
   },
   "source": [
    "Downloading the [Xenium Human Pancreatic Dataset](https://www.10xgenomics.com/products/xenium-human-pancreatic-dataset-explorer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Qjdt3f-U0_i9",
   "metadata": {
    "id": "Qjdt3f-U0_i9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: 无法创建目录\"data_xenium\": 文件已存在\n",
      "/home/cuisen/users/fengyun/segger/docs/notebooks/data_xenium\n",
      "Archive:  Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n",
      "  inflating: analysis_summary.html   \n",
      "  inflating: nucleus_boundaries.parquet  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuisen/miniconda3/envs/fengyun/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  inflating: cell_boundaries.parquet  \n",
      "  inflating: cells.csv.gz            \n",
      "  inflating: gene_panel.json         \n",
      "  inflating: morphology_focus.ome.tif  \n",
      " extracting: cell_feature_matrix.zarr.zip  \n",
      " extracting: transcripts.zarr.zip    \n",
      "  inflating: cell_feature_matrix.h5  \n",
      "  inflating: cells.parquet           \n",
      "  inflating: morphology_mip.ome.tif  \n",
      " extracting: analysis.zarr.zip       \n",
      " extracting: cells.zarr.zip          \n",
      "  inflating: morphology.ome.tif      \n",
      "  inflating: transcripts.csv.gz      \n",
      "  inflating: cell_boundaries.csv.gz  \n",
      "  inflating: nucleus_boundaries.csv.gz  \n",
      "  inflating: experiment.xenium       \n",
      "  inflating: transcripts.parquet     \n",
      "  inflating: analysis.tar.gz         \n",
      "  inflating: cell_feature_matrix.tar.gz  \n",
      "  inflating: he_imagealignment.csv   \n",
      "  inflating: cell_groups.csv         \n",
      "  inflating: gene_groups.csv         \n",
      "  inflating: metrics_summary.csv     \n",
      "/home/cuisen/users/fengyun/segger/docs/notebooks\n"
     ]
    }
   ],
   "source": [
    "!mkdir data_xenium\n",
    "%cd data_xenium\n",
    "#!wget https://cf.10xgenomics.com/samples/xenium/1.6.0/Xenium_V1_hPancreas_Cancer_Add_on_FFPE/Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n",
    "!unzip Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "trM8h-Ek16sJ",
   "metadata": {
    "id": "trM8h-Ek16sJ"
   },
   "outputs": [],
   "source": [
    "from segger.data.parquet.sample import STSampleParquet\n",
    "from segger.training.segger_data_module import SeggerDataModule\n",
    "from segger.training.train import LitSegger\n",
    "#from segger.prediction.predict_parquet import segment, load_model\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch import Trainer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db009015-c379-4f50-97ed-81dca9df28ac",
   "metadata": {
    "id": "db009015-c379-4f50-97ed-81dca9df28ac"
   },
   "source": [
    "# **1. Create your Segger Dataset**\n",
    "\n",
    "In this step, we generate the dataset required for Segger's cell segmentation tasks.\n",
    "\n",
    "Segger relies on spatial transcriptomics data, combining staining **boundaries** (e.g., nuclei or membrane stainings) and **transcripts** from single-cell resolved imaging datasets. These nuclei and transcript nodes are represented in a graph, and the spatial proximity of transcripts to nuclei is used to establish edges between them.\n",
    "\n",
    "To use Segger with a Xenium dataset, you need the **`transcripts.parquet`** and **`nucleus_boundaries.parquet`** (or **`cell_boundaries.parquet`**, in case the Xenium samples comes with the segmentation kit) files. The **transcripts** file contains spatial coordinates and information for each transcript, while the **boundaries** file defines the polygon boundaries of the nuclei or cells. These files enable segger to map transcripts to their respective nuclei and perform cell segmentation based on spatial relationships. Segger can also be extended to other platforms by modifying the column names or formats in the input files to match its expected structure, making it adaptable for various spatial transcriptomics technologies. See [this](https://github.com/EliHei2/segger_dev/tree/main/src/segger/data/parquet/_settings) for Xenium settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b090b",
   "metadata": {
    "id": "9d2b090b"
   },
   "source": [
    "### **1.1. Fast Dataset Creation with segger**\n",
    "\n",
    "Segger introduces a fast and efficient pipeline for processing spatial transcriptomics data. This method accelerates dataset creation, particularly for large datasets, by using **ND-tree-based spatial partitioning** and **parallel processing**. This results in a much faster preparation of the dataset, which is saved in PyTorch Geometric (PyG) format, similar to the previous method.\n",
    "\n",
    "**Note**: The previous dataset creation method will soon be deprecated in favor of this optimized pipeline.\n",
    "\n",
    "The pipeline requires the following inputs:\n",
    "\n",
    "- **base_dir**: The directory containing the raw dataset.\n",
    "- **data_dir**: The directory where the processed dataset (tiles in PyG format) will be saved.\n",
    "\n",
    "The core improvements in this method come from the use of **ND-tree partitioning**, which splits the data efficiently into spatial regions, and **parallel processing**, which speeds up the handling of these regions across multiple CPU cores. For example, using this pipeline, the Xenium Human Pancreatic Dataset can be processed in just a few minutes when running with 16 workers.\n",
    "\n",
    "Below is an example of how to create a dataset using the faster Segger pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e933ebf3",
   "metadata": {
    "id": "e933ebf3",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Directory 'data_segger/train_tiles/processed' must be empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m segger_data_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_segger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m sample \u001b[38;5;241m=\u001b[39m STSampleParquet(\n\u001b[1;32m      5\u001b[0m     base_dir\u001b[38;5;241m=\u001b[39mxenium_data_dir,\n\u001b[1;32m      6\u001b[0m     n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      7\u001b[0m     sample_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxenium\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m sample\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m     12\u001b[0m       data_dir\u001b[38;5;241m=\u001b[39msegger_data_dir,\n\u001b[1;32m     13\u001b[0m       k_bd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     14\u001b[0m       dist_bd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15.0\u001b[39m,\n\u001b[1;32m     15\u001b[0m       k_tx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     16\u001b[0m       dist_tx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m,\n\u001b[1;32m     17\u001b[0m       tile_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m,\n\u001b[1;32m     18\u001b[0m       tile_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m,\n\u001b[1;32m     19\u001b[0m       neg_sampling_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m,\n\u001b[1;32m     20\u001b[0m       frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     21\u001b[0m       val_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     22\u001b[0m       test_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/users/fengyun/segger/src/segger/data/parquet/sample.py:434\u001b[0m, in \u001b[0;36mSTSampleParquet.save\u001b[0;34m(self, data_dir, k_bd, dist_bd, k_tx, dist_tx, tile_size, tile_width, tile_height, neg_sampling_ratio, frac, val_prob, test_prob)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Setup directory structure to save tiles\u001b[39;00m\n\u001b[1;32m    433\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m Path(data_dir)\n\u001b[0;32m--> 434\u001b[0m STSampleParquet\u001b[38;5;241m.\u001b[39m_setup_directory(data_dir)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Function to parallelize over workers\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(region):\n",
      "File \u001b[0;32m~/users/fengyun/segger/src/segger/data/parquet/sample.py:339\u001b[0m, in \u001b[0;36mSTSampleParquet._setup_directory\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(tile_dir):\n\u001b[1;32m    338\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtile_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Directory 'data_segger/train_tiles/processed' must be empty."
     ]
    }
   ],
   "source": [
    "xenium_data_dir = Path('data_xenium')\n",
    "segger_data_dir = Path('data_segger')\n",
    "\n",
    "sample = STSampleParquet(\n",
    "    base_dir=xenium_data_dir,\n",
    "    n_workers=4,\n",
    "    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n",
    "    # weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available\n",
    ")\n",
    "\n",
    "sample.save(\n",
    "      data_dir=segger_data_dir,\n",
    "      k_bd=3,\n",
    "      dist_bd=15.0,\n",
    "      k_tx=3,\n",
    "      dist_tx=5.0,\n",
    "      tile_width=120,\n",
    "      tile_height=120,\n",
    "      neg_sampling_ratio=5.0,\n",
    "      frac=1.0,\n",
    "      val_prob=0.1,\n",
    "      test_prob=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab27f9a",
   "metadata": {
    "id": "6ab27f9a"
   },
   "source": [
    "#### **Parameters**\n",
    "Here is a complete list of parameters you can use to control the dataset creation process:\n",
    "\n",
    "- **--base_dir**: Directory containing the raw spatial transcriptomics dataset.\n",
    "- **--data_dir**: Directory where the processed Segger dataset (in PyG format) will be saved.\n",
    "- **--sample_type**: (Optional) Specifies the type of dataset (e.g., \"xenium\" or \"merscope\"). Defaults to None.\n",
    "- **--scrnaseq_file**: Path to the scRNAseq file (default: None).\n",
    "- **--celltype_column**: Column name for cell type annotations in the scRNAseq file (default: None).\n",
    "- **--k_bd**: Number of nearest neighbors for boundary nodes (default: 3).\n",
    "- **--dist_bd**: Maximum distance for boundary neighbors (default: 15.0).\n",
    "- **--k_tx**: Number of nearest neighbors for transcript nodes (default: 3).\n",
    "- **--dist_tx**: Maximum distance for transcript neighbors (default: 5.0).\n",
    "- **--tile_size**: Specifies the size of the tile. If provided, it overrides both tile_width and tile_height.\n",
    "- **--tile_width**: Width of the tiles in pixels (ignored if tile_size is provided).\n",
    "- **--tile_height**: Height of the tiles in pixels (ignored if tile_size is provided).\n",
    "- **--neg_sampling_ratio**: Ratio of negative samples (default: 5.0).\n",
    "- **--frac**: Fraction of the dataset to process (default: 1.0).\n",
    "- **--val_prob**: Proportion of data used for validation split (default: 0.1).\n",
    "- **--test_prob**: Proportion of data used for testing split (default: 0.2).\n",
    "- **--n_workers**: Number of workers for parallel processing (default: 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70755046",
   "metadata": {},
   "source": [
    "### **1.2. Using custom gene embeddings**\n",
    "\n",
    "In the default mode, segger initially tokenizes transcripts based on their gene type simply in a one-hot manner. However, one can use other genes embeddings (e.g., pre-trained embeddings). The following example shows how one can employ a cell-type-annotated scRNAseq reference of the same tissue type (not necessary same sample or experiment) to embed genes based on their abaundance in different cell types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf18259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'my_scRNAseq_file.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m scrnaseq_file \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_scRNAseq_file.h5ad\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m celltype_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcelltype_column\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m gene_celltype_abundance_embedding \u001b[38;5;241m=\u001b[39m calculate_gene_celltype_abundance_embedding(\n\u001b[0;32m----> 5\u001b[0m     sc\u001b[38;5;241m.\u001b[39mread(scrnaseq_file),\n\u001b[1;32m      6\u001b[0m     celltype_column\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m sample \u001b[38;5;241m=\u001b[39m STSampleParquet(\n\u001b[1;32m     10\u001b[0m     base_dir\u001b[38;5;241m=\u001b[39mxenium_data_dir,\n\u001b[1;32m     11\u001b[0m     n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     12\u001b[0m     sample_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxenium\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     weights\u001b[38;5;241m=\u001b[39mgene_celltype_abundance_embedding, \n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:82\u001b[0m, in \u001b[0;36mlegacy_api.<locals>.wrapper.<locals>.fn_compatible\u001b[0;34m(*args_all, **kw)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_compatible\u001b[39m(\u001b[38;5;241m*\u001b[39margs_all: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_all) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_positional:\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs_all, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     84\u001b[0m     args_pos: P\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m     85\u001b[0m     args_pos, args_rest \u001b[38;5;241m=\u001b[39m args_all[:n_positional], args_all[n_positional:]\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/scanpy/readwrite.py:143\u001b[0m, in \u001b[0;36mread\u001b[0;34m(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m filename \u001b[38;5;241m=\u001b[39m Path(filename)  \u001b[38;5;66;03m# allow passing strings\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_filename(filename):\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read(\n\u001b[1;32m    144\u001b[0m         filename,\n\u001b[1;32m    145\u001b[0m         backed\u001b[38;5;241m=\u001b[39mbacked,\n\u001b[1;32m    146\u001b[0m         sheet\u001b[38;5;241m=\u001b[39msheet,\n\u001b[1;32m    147\u001b[0m         ext\u001b[38;5;241m=\u001b[39mext,\n\u001b[1;32m    148\u001b[0m         delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[1;32m    149\u001b[0m         first_column_names\u001b[38;5;241m=\u001b[39mfirst_column_names,\n\u001b[1;32m    150\u001b[0m         backup_url\u001b[38;5;241m=\u001b[39mbackup_url,\n\u001b[1;32m    151\u001b[0m         cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m    152\u001b[0m         cache_compression\u001b[38;5;241m=\u001b[39mcache_compression,\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# generate filename and read to dict\u001b[39;00m\n\u001b[1;32m    156\u001b[0m filekey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(filename)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/scanpy/readwrite.py:787\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sheet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 787\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m read_h5ad(filename, backed\u001b[38;5;241m=\u001b[39mbacked)\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m         logg\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msheet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/anndata/_io/h5ad.py:234\u001b[0m, in \u001b[0;36mread_h5ad\u001b[0;34m(filename, backed, as_sparse, as_sparse_fmt, chunk_size)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently only `X` and `raw/X` can be read as sparse.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         )\n\u001b[1;32m    230\u001b[0m rdasp \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    231\u001b[0m     read_dense_as_sparse, sparse_format\u001b[38;5;241m=\u001b[39mas_sparse_fmt, axis_chunk\u001b[38;5;241m=\u001b[39mchunk_size\n\u001b[1;32m    232\u001b[0m )\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcallback\u001b[39m(func, elem_name: \u001b[38;5;28mstr\u001b[39m, elem, iospec):\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m iospec\u001b[38;5;241m.\u001b[39mencoding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manndata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'my_scRNAseq_file.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from segger.data.utils import calculate_gene_celltype_abundance_embedding\n",
    "scrnaseq_file = Path('my_scRNAseq_file.h5ad')\n",
    "celltype_column = 'celltype_column'\n",
    "gene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(\n",
    "    sc.read(scrnaseq_file),\n",
    "    celltype_column\n",
    ")\n",
    "\n",
    "sample = STSampleParquet(\n",
    "    base_dir=xenium_data_dir,\n",
    "    n_workers=4,\n",
    "    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n",
    "    weights=gene_celltype_abundance_embedding, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962e4b8-4028-4683-9b75-d674fa6fb01d",
   "metadata": {
    "id": "9962e4b8-4028-4683-9b75-d674fa6fb01d"
   },
   "source": [
    "# **2. Train your Segger Model**\n",
    "\n",
    "The Segger model training process begins after the dataset has been created. This model is a **heterogeneous graph neural network (GNN)** designed to segment single cells by leveraging both nuclei and transcript data.\n",
    "\n",
    "Segger uses graph attention layers to propagate information across nodes (nuclei and transcripts) and refine cell boundaries. The model architecture includes initial embedding layers, attention-based graph convolutions, and residual connections for stable learning.\n",
    "\n",
    "Segger leverages the **PyTorch Lightning** framework to streamline the training and evaluation of its graph neural network (GNN). PyTorch Lightning simplifies the training process by abstracting away much of the boilerplate code, allowing users to focus on model development and experimentation. It also supports multi-GPU training, mixed-precision, and efficient scaling, making it an ideal framework for training complex models like Segger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db89cb4-d0eb-426a-a71f-d127926fa412",
   "metadata": {
    "id": "4db89cb4-d0eb-426a-a71f-d127926fa412"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuisen/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "/home/cuisen/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/cuisen/miniconda3/envs/fengyun/lib/python3.11/ ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from segger.models.segger_model import Segger\n",
    "from torch_geometric.nn import to_hetero\n",
    "# Base directory to store Pytorch Lightning models\n",
    "models_dir = Path('models')\n",
    "\n",
    "# Initialize the Lightning data module\n",
    "dm = SeggerDataModule(\n",
    "    data_dir=segger_data_dir,\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "num_tx_tokens = 500\n",
    "\n",
    "# If you use custom gene embeddings, use the following two lines instead:\n",
    "# num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes\n",
    "\n",
    "\n",
    "model = Segger(\n",
    "    # is_token_based=is_token_based,\n",
    "    num_tx_tokens=num_tx_tokens,\n",
    "    init_emb=8,\n",
    "    hidden_channels=64,\n",
    "    out_channels=16,\n",
    "    heads=4,\n",
    "    num_mid_layers=3,\n",
    ")\n",
    "model = to_hetero(model, ([\"tx\", \"bd\"], [(\"tx\", \"belongs\", \"bd\"), (\"tx\", \"neighbors\", \"tx\")]), aggr=\"sum\")\n",
    "\n",
    "batch = dm.train[0]\n",
    "model.forward(batch.x_dict, batch.edge_index_dict)\n",
    "# Wrap the model in LitSegger\n",
    "ls = LitSegger(model=model)\n",
    "\n",
    "# Initialize the Lightning trainer\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    strategy='auto',\n",
    "    precision='16-mixed',\n",
    "    devices=1, # set higher number if more gpus are available\n",
    "    max_epochs=100,\n",
    "    default_root_dir=models_dir,\n",
    "    logger=CSVLogger(models_dir),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207864b8-7e52-4add-a4a2-e95a4debdc06",
   "metadata": {
    "id": "207864b8-7e52-4add-a4a2-e95a4debdc06",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model     | GraphModule       | 876 K  | train\n",
      "1 | criterion | BCEWithLogitsLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "876 K     Trainable params\n",
      "0         Non-trainable params\n",
      "876 K     Total params\n",
      "3.508     Total estimated model params size (MB)\n",
      "54        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/cuisen/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'model' parameter because it is not possible to safely dump to YAML.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552465ae22744e5ea09b679ea77c83d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                           |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f758dd23964d308ccfa6e141c3946a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                  |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.75 MiB is free. Process 68034 has 77.48 GiB memory in use. Including non-PyTorch memory, this process has 1.63 GiB memory in use. Of the allocated memory 1.11 GiB is allocated by PyTorch, and 25.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mls,\n\u001b[1;32m      4\u001b[0m     datamodule\u001b[38;5;241m=\u001b[39mdm\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    563\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_optimization\u001b[38;5;241m.\u001b[39mrun(trainer\u001b[38;5;241m.\u001b[39moptimizers[\u001b[38;5;241m0\u001b[39m], batch_idx, kwargs)\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         closure()\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step(batch_idx, closure)\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\n\u001b[1;32m    271\u001b[0m     trainer,\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_step\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    273\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch,\n\u001b[1;32m    274\u001b[0m     batch_idx,\n\u001b[1;32m    275\u001b[0m     optimizer,\n\u001b[1;32m    276\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    277\u001b[0m )\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:176\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    179\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1273\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39moptimizer_closure)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/amp.py:79\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n\u001b[1;32m     82\u001b[0m skip_unscaling \u001b[38;5;241m=\u001b[39m closure_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_fn()\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/users/fengyun/segger/src/segger/training/train.py:76\u001b[0m, in \u001b[0;36mLitSegger.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mDefines the training step.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    The loss value for the current training step.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Forward pass to get the logits\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch\u001b[38;5;241m.\u001b[39mx_dict, batch\u001b[38;5;241m.\u001b[39medge_index_dict)\n\u001b[1;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(z[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx\u001b[39m\u001b[38;5;124m\"\u001b[39m], z[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mt())\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Get edge labels and logits\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch/fx/graph_module.py:822\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch/fx/graph_module.py:400\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch/fx/graph_module.py:387\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls, obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m<eval_with_key>.3:58\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m relu_3__bd \u001b[38;5;241m=\u001b[39m conv_mid_layers_1__bd\u001b[38;5;241m.\u001b[39mrelu();  conv_mid_layers_1__bd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     57\u001b[0m conv_mid_layers_2__bd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_mid_layers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtx__belongs__bd((relu_3__tx, relu_3__bd), edge_index__tx__belongs__bd);  relu_3__bd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m conv_mid_layers_2__tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_mid_layers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtx__neighbors__tx(relu_3__tx, edge_index__tx__neighbors__tx);  relu_3__tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m relu_4__tx \u001b[38;5;241m=\u001b[39m conv_mid_layers_2__tx\u001b[38;5;241m.\u001b[39mrelu();  conv_mid_layers_2__tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m relu_4__bd \u001b[38;5;241m=\u001b[39m conv_mid_layers_2__bd\u001b[38;5;241m.\u001b[39mrelu();  conv_mid_layers_2__bd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch_geometric/nn/conv/gatv2_conv.py:325\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_updater(edge_index, x\u001b[38;5;241m=\u001b[39m(x_l, x_r),\n\u001b[1;32m    326\u001b[0m                           edge_attr\u001b[38;5;241m=\u001b[39medge_attr)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: PairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39m(x_l, x_r), alpha\u001b[38;5;241m=\u001b[39malpha)\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater_uehwq6og.py:145\u001b[0m, in \u001b[0;36medge_updater\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21medge_updater\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     edge_index: Union[Tensor, SparseTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     size: Size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    143\u001b[0m     mutable_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(edge_index, size)\n\u001b[0;32m--> 145\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_collect(\n\u001b[1;32m    146\u001b[0m         edge_index,\n\u001b[1;32m    147\u001b[0m         x,\n\u001b[1;32m    148\u001b[0m         edge_attr,\n\u001b[1;32m    149\u001b[0m         mutable_size,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Begin Edge Update Forward Pre Hook #######################################\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater_uehwq6og.py:109\u001b[0m, in \u001b[0;36medge_collect\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_x_1, Tensor):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, \u001b[38;5;241m1\u001b[39m, _x_1)\n\u001b[0;32m--> 109\u001b[0m     x_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_select(_x_1, edge_index_i)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     x_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:267\u001b[0m, in \u001b[0;36mMessagePassing._index_select\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_select_safe(src, index)\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:290\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m that are larger \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/fengyun/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:271\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_index_select_safe\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: Tensor, index: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.75 MiB is free. Process 68034 has 77.48 GiB memory in use. Including non-PyTorch memory, this process has 1.63 GiB memory in use. Of the allocated memory 1.11 GiB is allocated by PyTorch, and 25.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "trainer.fit(\n",
    "    model=ls,\n",
    "    datamodule=dm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6214a79",
   "metadata": {},
   "source": [
    "Key parameters for training:\n",
    "- **`--data_dir`**: Directory containing the training data.\n",
    "- **`--model_dir`**: Directory in which to store models.\n",
    "- **`--epochs`**: Specifies the number of training epochs.\n",
    "- **`--batch_size`**: Batch sizes for training and validation data.\n",
    "- **`--learning_rate`**: The initial learning rate for the optimizer.\n",
    "- **`--hidden_channels`**: Number of hidden channels in the GNN layers.\n",
    "- **`--heads`**: Number of attention heads used in each graph convolutional layer.\n",
    "- **`--init_emb`**: Sets the dimensionality of the initial embeddings applied to the input node features (e.g., transcripts). A higher embedding dimension may capture more feature complexity but also requires more computation.\n",
    "- **`--out_channels`**: Specifies the number of output channels after the final graph attention layer, e.g. the final learned representations of the graph nodes.\n",
    "\n",
    "Additional Options for Training the Segger Model:\n",
    "\n",
    "- **`--aggr`**: This option controls the aggregation method used in the graph convolution layers.\n",
    "- **`--accelerator`**: Controls the hardware used for training, such as `cuda` for GPU training. This enables Segger to leverage GPU resources for faster training, especially useful for large datasets.\n",
    "- **`--strategy`**: Defines the distributed training strategy, with `auto` allowing PyTorch Lightning to automatically configure the best strategy based on the hardware setup.\n",
    "- **`--precision`**: Enables mixed precision training (e.g., `16-mixed`), which can speed up training and reduce memory usage while maintaining accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d20c6-ca16-4beb-b627-afb41e3fb491",
   "metadata": {
    "id": "9a7d20c6-ca16-4beb-b627-afb41e3fb491"
   },
   "source": [
    "### *Troubleshooting #1*\n",
    "\n",
    "In the cell below, we are visualizing key metrics from the model training and validation process. The plot displays **training loss**, **validation loss**, **F1 validation score**, and **AUROC validation score** over training steps. We expect to see the loss curves decreasing over time, signaling the model's improvement, and the F1 and AUROC scores increasing, reflecting improved segmentation performance as the model learns.\n",
    "\n",
    "If training is not working effectively, you might observe the following in the plot displaying **training loss**, **validation loss**, **F1 score**, and **AUROC**:\n",
    "\n",
    "- **Training loss not decreasing**: If the training loss remains high or fluctuates without a consistent downward trend, this indicates that the model is not learning effectively from the training data.\n",
    "- **Validation loss decreases, then increases**: If validation loss decreases initially but starts to increase while training loss continues to drop, this could be a sign of **overfitting**, where the model is performing well on the training data but not generalizing to the validation data.\n",
    "- **F1 score and AUROC not improving**: If these metrics remain flat or show inconsistent improvement, the model may be struggling to correctly segment cells or classify transcripts, indicating an issue with learning performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9c1a4-3898-407d-ac0f-f98b13694593",
   "metadata": {
    "id": "43a9c1a4-3898-407d-ac0f-f98b13694593",
    "outputId": "70ba8e1b-7814-497a-c8b6-8aa7295cd4d9"
   },
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "model_version = 0  # 'v_num' from training output above\n",
    "model_path = models_dir / 'lightning_logs' / f'version_{model_version}'\n",
    "metrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(2,2))\n",
    "\n",
    "for col in metrics.columns.difference(['epoch']):\n",
    "    metric = metrics[col].dropna()\n",
    "    ax.plot(metric.index, metric.values, label=col)\n",
    "\n",
    "ax.legend(loc=(1, 0.33))\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73687e1-ee8f-46e9-8bd2-1ddc571ef94b",
   "metadata": {
    "id": "e73687e1-ee8f-46e9-8bd2-1ddc571ef94b"
   },
   "source": [
    "# **3. Make Predictions**\n",
    "\n",
    "Once the Segger model is trained, it can be used to make predictions on seen (partially trained) data or be transfered to unseen data. This step involves using a trained checkpoint to predict cell boundaries and refine transcript-nuclei associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807abf3",
   "metadata": {
    "id": "9807abf3"
   },
   "source": [
    "#### **Requirements for the Faster Prediction Pipeline**\n",
    "The pipeline requires the following inputs:\n",
    "\n",
    "- **segger_data_dir**: The directory containing the processed Segger dataset (in PyG format).\n",
    "- **models_dir**: The directory containing the trained Segger model checkpoints.\n",
    "- **benchmarks_dir**: The directory where the segmentation results will be saved.\n",
    "- **transcripts_file**: Path to the file containing the transcript data for prediction.\n",
    "\n",
    "#### **Running the Faster Prediction Pipeline**\n",
    "Below is an example of how to run the faster Segger prediction pipeline using the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PEOtAs-t9CiY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEOtAs-t9CiY",
    "outputId": "8b7a5375-9ebc-4bb4-9421-254410319120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting segmentation for segger_embedding_1001...\n"
     ]
    }
   ],
   "source": [
    "dm = SeggerDataModule(\n",
    "    data_dir='data_segger',\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "model_version = 0\n",
    "model_path = Path('models') / \"lightning_logs\" / f\"version_{model_version}\"\n",
    "model = load_model(model_path / \"checkpoints\")\n",
    "\n",
    "receptive_field = {'k_bd': 4, 'dist_bd': 12, 'k_tx': 15, 'dist_tx': 3}\n",
    "\n",
    "segment(\n",
    "    model,\n",
    "    dm,\n",
    "    save_dir='benchmarks',\n",
    "    seg_tag='segger_output',\n",
    "    transcript_file='data_xenium/transcripts.parquet',\n",
    "    receptive_field=receptive_field,\n",
    "    min_transcripts=5,\n",
    "    cell_id_col='segger_cell_id',\n",
    "    use_cc=False,\n",
    "    knn_method='kd_tree',\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a823035",
   "metadata": {
    "id": "0a823035"
   },
   "source": [
    "#### **Parameters**\n",
    "Here is a detailed explanation of each parameter used in the faster prediction pipeline:\n",
    "\n",
    "- **--segger_data_dir**: The directory containing the processed Segger dataset, saved as PyTorch Geometric data objects, that will be used for prediction.\n",
    "- **--models_dir**: The directory containing the trained Segger model checkpoints. These checkpoints store the learned weights required for making predictions.\n",
    "- **--benchmarks_dir**: The directory where the segmentation results will be saved.\n",
    "- **--transcripts_file**: Path to the *transcripts.parquet* file.\n",
    "- **--batch_size**: Specifies the batch size for processing during prediction. Larger batch sizes speed up inference but use more memory (default: 1).\n",
    "- **--num_workers**: Number of workers to use for parallel data loading (default: 1).\n",
    "- **--model_version**: Version of the trained model to load for predictions, based on the version number from the training logs (default: 0).\n",
    "- **--save_tag**: A tag used to name and organize the segmentation results (default: segger_embedding).\n",
    "- **--min_transcripts**: The minimum number of transcripts required for segmentation (default: 5).\n",
    "- **--cell_id_col**: The name of the column that stores the cell IDs (default: segger_cell_id).\n",
    "- **--use_cc**: Enables the use of connected components (CC) for grouping transcripts that are not associated with any nucleus (default: False).\n",
    "- **--knn_method**: Method for KNN (K-Nearest Neighbors) computation. Only option is \"cuda\" for this pipeline (default: cuda).\n",
    "- **--file_format**: The format for saving the output segmentation data. Only option is \"anndata\" for this pipeline (default: anndata).\n",
    "- **--k_bd**: Number of nearest neighbors for boundary nodes during segmentation (default: 4).\n",
    "- **--dist_bd**: Maximum distance for boundary nodes during segmentation (default: 12.0).\n",
    "- **--k_tx**: Number of nearest neighbors for transcript nodes during segmentation (default: 5).\n",
    "- **--dist_tx**: Maximum distance for transcript nodes during segmentation (default: 5.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0917be9-4e82-4ba5-869d-5a9203721699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T23:06:23.977884Z",
     "iopub.status.busy": "2024-09-11T23:06:23.977517Z"
    },
    "id": "b0917be9-4e82-4ba5-869d-5a9203721699"
   },
   "source": [
    "### *Troubleshooting #2*\n",
    "\n",
    "In the cell below, we are visualizing the distribution of **Segger similarity scores** using a histogram. The **Segger similarity score** reflects how closely transcripts are associated with their respective nuclei in the segmentation process. **Higher scores** indicate stronger associations between transcripts and their nuclei, suggesting more accurate cell boundaries. **Lower scores** might indicate weaker associations, which could highlight potential segmentation errors or challenging regions in the data. We expect to see a large number of the scores clustering toward higher values, which would indicate strong overall performance of the model in associating transcripts with nuclei.\n",
    "\n",
    "The following would indicate potential issues with the model's predictions:\n",
    "\n",
    "- **A very large portion of scores near zero**: If many scores are concentrated at the lower end of the scale, this suggests that the model is frequently failing to associate transcripts with their corresponding nuclei, indicating poor segmentation quality.\n",
    "- **No clear peak in the distribution**: If the histogram is flat or shows a wide, spread-out distribution, this could indicate that the model is struggling to consistently assign similarity scores, which may be a sign that the training process did not optimize the model correctly.\n",
    "\n",
    "Both cases would suggest that the model requires further tuning, such as adjusting hyperparameters, data preprocessing, or the training procedure (see below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450d3ca-2876-4f48-be89-761147b17387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T22:07:04.216273Z",
     "iopub.status.busy": "2024-09-11T22:07:04.215965Z",
     "iopub.status.idle": "2024-09-11T22:07:08.177601Z",
     "shell.execute_reply": "2024-09-11T22:07:08.177158Z",
     "shell.execute_reply.started": "2024-09-11T22:07:04.216257Z"
    },
    "id": "a450d3ca-2876-4f48-be89-761147b17387",
    "outputId": "0576e0b8-4823-4701-b661-dc6b513841f2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(2,2))\n",
    "sns.histplot(\n",
    "    segmentation['score'],\n",
    "    bins=50,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Segger Similarity Score')\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492fb96-bf8e-49d5-b40e-7e6b3f871bbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T22:34:15.990223Z",
     "iopub.status.busy": "2024-09-11T22:34:15.988880Z"
    },
    "id": "5492fb96-bf8e-49d5-b40e-7e6b3f871bbe"
   },
   "source": [
    "#### The Importance of the Receptive Field in Segger\n",
    "\n",
    "The **receptive field** is a critical parameter in Segger, as it directly influences how the model interprets the spatial relationships between **transcripts** and **nuclei**. In the context of spatial transcriptomics, the receptive field determines the size of the neighborhood that each node (representing transcripts or nuclei) can \"see\" during graph construction and model training. Segger is particularly sensitive to the size of the receptive field because it affects the model's ability to propagate information across the graph. If the receptive field is too small, the model may fail to capture sufficient context for correct cell boundary delineation. Conversely, a very large receptive field may introduce noise by linking unrelated or distant nodes, reducing segmentation accuracy.\n",
    "\n",
    "#### Parameters affecting the receptive field in Segger:\n",
    "- **`--r`**: This parameter defines the radius used when connecting transcripts to nuclei. A larger `r` expands the receptive field, linking more distant nodes. Fine-tuning this parameter helps ensure that Segger captures the right level of spatial interaction in the dataset.\n",
    "- **`--k_bd` and `--k_tx`**: These control the number of nearest neighbors (nuclei and transcripts, respectively) considered in the graph. By increasing these values, the receptive field is effectively broadened, allowing more nodes to contribute to the information propagation.\n",
    "- **`--dist_bd` and `--dist_tx`**: These parameters specify the maximum distances used to connect nuclei (`dist_bd`) and transcripts (`dist_tx`) to their neighbors during graph construction. They directly affect the receptive field by defining the cut-off distance for forming edges in the graph. Larger distance values expand the receptive field, connecting nodes that are further apart spatially. Careful tuning of these values is necessary to ensure that Segger captures relevant spatial relationships without introducing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece1ac0-0708-45e2-87fc-1b25782831f8",
   "metadata": {
    "id": "7ece1ac0-0708-45e2-87fc-1b25782831f8"
   },
   "source": [
    "# **4. Tune Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b8288-5287-4d10-a206-e68c0e4731c6",
   "metadata": {
    "id": "896b8288-5287-4d10-a206-e68c0e4731c6"
   },
   "source": [
    "### Evaluating Receptive Field Parameters with Grid Search\n",
    "\n",
    "To evaluate the impact of different receptive field parameters in Segger, we use a **grid search** approach. The parameters `k_bd`, `k_tx`, `dist_bd`, and `dist_tx` (which control the number of neighbors and distances for nuclei and transcripts) are explored through various configurations defined in `param_space`. Each combination of these parameters is passed to the `trainable` function, which creates the dataset, trains the model, and makes predictions based on the specified receptive field.\n",
    "\n",
    "For each parameter combination:\n",
    "1. A dataset is created with the specified receptive field.\n",
    "2. The Segger model is trained on this dataset.\n",
    "3. Predictions are made, and segmentation results are evaluated using the custom `evaluate` function. This function computes metrics like the fraction of assigned transcripts and average cell sizes.\n",
    "\n",
    "The results from each configuration are saved, allowing us to compare how different receptive field settings impact the model’s performance. This process enables a thorough search of the parameter space, optimizing the model for accurate segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1a7a8-acb2-4aae-8ae4-8aa9a4196717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T01:10:47.781418Z",
     "iopub.status.busy": "2024-09-12T01:10:47.781067Z",
     "iopub.status.idle": "2024-09-12T01:10:48.706615Z",
     "shell.execute_reply": "2024-09-12T01:10:48.706194Z",
     "shell.execute_reply.started": "2024-09-12T01:10:47.781401Z"
    },
    "id": "b0c1a7a8-acb2-4aae-8ae4-8aa9a4196717"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0803c-e58d-4f43-9627-d2c1ab187d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T01:16:31.976312Z",
     "iopub.status.busy": "2024-09-12T01:16:31.975947Z",
     "iopub.status.idle": "2024-09-12T01:16:33.168389Z",
     "shell.execute_reply": "2024-09-12T01:16:33.167956Z",
     "shell.execute_reply.started": "2024-09-12T01:16:31.976295Z"
    },
    "id": "0bd0803c-e58d-4f43-9627-d2c1ab187d5e"
   },
   "outputs": [],
   "source": [
    "tuning_dir = Path('path/to/tutorial/tuning/')\n",
    "sampling_rate = 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879a0b5-150c-4240-99ec-81075855aa52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T01:16:33.169525Z",
     "iopub.status.busy": "2024-09-12T01:16:33.169189Z",
     "iopub.status.idle": "2024-09-12T01:16:34.147222Z",
     "shell.execute_reply": "2024-09-12T01:16:34.146804Z",
     "shell.execute_reply.started": "2024-09-12T01:16:33.169508Z"
    },
    "id": "b879a0b5-150c-4240-99ec-81075855aa52",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fixed function arguments used for each trial\n",
    "transcripts_path = xenium_data_dir / 'transcripts.parquet'\n",
    "\n",
    "boundaries_path = xenium_data_dir / 'nucleus_boundaries.parquet'\n",
    "\n",
    "dataset_kwargs = dict(\n",
    "    x_size=80, y_size=80, d_x=80, d_y=80, margin_x=10, margin_y=10,\n",
    "    num_workers=4, sampling_rate=sampling_rate,\n",
    ")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    metadata=(['tx', 'bd'], [('tx', 'belongs', 'bd'), ('tx', 'neighbors', 'tx')]),\n",
    "    num_tx_tokens=500, init_emb=8, hidden_channels=32, out_channels=8,\n",
    "    heads=2, num_mid_layers=2, aggr='sum',\n",
    ")\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    accelerator='cuda', strategy='auto', precision='16-mixed', devices=1,\n",
    "    max_epochs=100,\n",
    ")\n",
    "\n",
    "predict_kwargs = dict(score_cut=0.2, use_cc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd831c9-3a50-4e3b-97d3-3c152ae01188",
   "metadata": {
    "id": "fbd831c9-3a50-4e3b-97d3-3c152ae01188",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def trainable(config):\n",
    "\n",
    "    receptive_field = {k: config[k] for k in ['k_bd', 'k_tx', 'dist_bd', 'dist_tx']}\n",
    "\n",
    "    # Dataset creation\n",
    "    xs = XeniumSample(verbose=False)\n",
    "    xs.set_file_paths(transcripts_path, boundaries_path)\n",
    "    xs.set_metadata()\n",
    "    try:\n",
    "        xs.save_dataset_for_segger(\n",
    "            processed_dir=config['data_dir'],\n",
    "            receptive_field=receptive_field,\n",
    "            **dataset_kwargs,\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Model training\n",
    "    ls = LitSegger(**model_kwargs)\n",
    "    dm = SeggerDataModule(\n",
    "        data_dir=config['data_dir'],\n",
    "        batch_size=2,\n",
    "        num_workers=dataset_kwargs['num_workers'],\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        default_root_dir=config['model_dir'],\n",
    "        logger=CSVLogger(config['model_dir']),\n",
    "        **trainer_kwargs,\n",
    "    )\n",
    "    trainer.fit(model=ls, datamodule=dm)\n",
    "\n",
    "    segmentation = predict(\n",
    "        load_model(config['model_dir']/'lightning_logs/version_0/checkpoints'),\n",
    "        dm.train_dataloader(),\n",
    "        receptive_field=receptive_field,\n",
    "        **predict_kwargs,\n",
    "    )\n",
    "\n",
    "    metrics = evaluate(segmentation)\n",
    "\n",
    "\n",
    "def evaluate(segmentation: pd.DataFrame, score_cut: float) -> pd.Series:\n",
    "\n",
    "    assigned = segmentation['score'] > score_cut\n",
    "    metrics = pd.Series(dtype=float)\n",
    "    metrics['frac_assigned'] = assigned.mean()\n",
    "    cell_sizes = segmentation.groupby(assigned)['segger_cell_id'].value_counts()\n",
    "    assigned_avg = 0 if True not in cell_sizes.index else cell_sizes[True].mean()\n",
    "    cc_avg = 0 if False not in cell_sizes.index else cell_sizes[False].mean()\n",
    "    metrics['cell_size_assigned'] = assigned_avg\n",
    "    metrics['cell_size_cc'] = cc_avg\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dcc9a-3a06-4b84-a487-59a768eed5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T01:16:35.184598Z",
     "iopub.status.busy": "2024-09-12T01:16:35.184180Z",
     "iopub.status.idle": "2024-09-12T01:19:55.171470Z",
     "shell.execute_reply": "2024-09-12T01:19:55.170810Z",
     "shell.execute_reply.started": "2024-09-12T01:16:35.184582Z"
    },
    "id": "ba2dcc9a-3a06-4b84-a487-59a768eed5d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    \"k_bd\": [3, 5, 10],\n",
    "    \"dist_bd\": [5, 10, 15, 20],\n",
    "    \"k_tx\": [3, 5, 10],\n",
    "    \"dist_tx\": [3, 5, 10],\n",
    "}\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for params in itertools.product(*param_space.values()):\n",
    "\n",
    "    config = dict(zip(param_space.keys(), params))\n",
    "\n",
    "    # Setup directories\n",
    "    trial_dir = tuning_dir / '_'.join([f'{k}={v}' for k, v in config.items()])\n",
    "\n",
    "    data_dir = trial_dir / 'segger_data'\n",
    "    data_dir.mkdir(exist_ok=True, parents=True)\n",
    "    config['data_dir'] = data_dir\n",
    "\n",
    "    model_dir = trial_dir / 'models'\n",
    "    model_dir.mkdir(exist_ok=True, parents=True)\n",
    "    config['model_dir'] = model_dir\n",
    "\n",
    "    segmentation = trainable(config)\n",
    "    trial = evaluate(segmentation, predict_kwargs['score_cut'])\n",
    "    trial = pd.concat([pd.Series(config), trial])\n",
    "    metrics.append(trial)\n",
    "\n",
    "metrics = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa5570-ada2-4102-aae0-a3830d304c5f",
   "metadata": {
    "id": "dcfa5570-ada2-4102-aae0-a3830d304c5f"
   },
   "source": [
    "### Interpreting Output Metrics\n",
    "\n",
    "The key output metrics include:\n",
    "- **`frac_assigned`**: The fraction of transcripts that were successfully assigned to a cell. A higher value indicates that the model is doing a good job associating transcripts with nuclei, which is a strong indicator of successful segmentation.\n",
    "- **`cell_size_assigned`**: The average size of cells that have assigned transcripts. This helps assess how well the model is predicting cell boundaries, with unusually large or small values indicating potential issues with segmentation accuracy.\n",
    "- **`cell_size_cc`**: The average size of connected components that were not assigned to a cell (i.e., nucleus-less regions). Large values here may suggest that transcripts are being incorrectly grouped together in the absence of a nucleus, which could indicate problems with the receptive field parameters or the segmentation process.\n",
    "\n",
    "These metrics illuminate the effectiveness of the model by highlighting both the success in associating transcripts with cells and potential areas where the model may need further tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89aed4-c53b-460f-8a6f-f690920b6829",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T01:19:55.171961Z",
     "iopub.status.idle": "2024-09-12T01:19:55.172161Z",
     "shell.execute_reply": "2024-09-12T01:19:55.172071Z",
     "shell.execute_reply.started": "2024-09-12T01:19:55.172062Z"
    },
    "id": "1a89aed4-c53b-460f-8a6f-f690920b6829"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fengyun segger",
   "language": "python",
   "name": "fengyun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
